{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5668eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4826ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定隨機種子，確保實驗可重現\n",
    "seed=42\n",
    "\n",
    "# 設定資料路徑\n",
    "file_path = './cases/bert_finetune/reviews.txt'\n",
    "\n",
    "# 讀取資料\n",
    "with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "    # 將每一行資料以 list 型態回傳\n",
    "    lines = file.readlines()\n",
    "\n",
    "    # 整合訓練資料\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # 逐行讀取資料\n",
    "    for line in lines:\n",
    "        # 每一行資料的 tab (\\t) 作為分隔符號\n",
    "        parts = line.strip().split('\\t')\n",
    "\n",
    "        # 確保每一行資料都有兩個部分\n",
    "        if len(parts) == 2:\n",
    "            sentences.append(parts[0].strip('\"'))\n",
    "            labels.append(int(parts[1]))\n",
    "        else:\n",
    "            print(f'格式錯誤的行號: {line}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b0a5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分數據集為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sentences, \n",
    "    labels, \n",
    "    test_size=0.2, \n",
    "    random_state=seed, \n",
    "    stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a47fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 TF-IDF 物件\n",
    "# tokenizer 預設是 None，表示使用內建的斷詞方式，這裡我們使用 jieba.lcut 來進行中文斷詞\n",
    "# token_pattern=None 是為了讓自訂的 tokenizer 生效，否則預設會以英文單字的正則表達式來切分\n",
    "# ngram_range 可調整 n-gram 範圍，\n",
    "# (1,1) 表示只使用 unigram，\n",
    "# (2,2) 表示只使用 bigram, \n",
    "# (1,2) 表示同時使用 unigram 和 bigram\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=jieba.lcut, \n",
    "    token_pattern=None, \n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "'''\n",
    "fit(...)：從資料學出向量化規則\n",
    "- 建詞表（vocabulary：有哪些詞/詞組會成為特徵）\n",
    "- 計算 IDF（每個詞的 inverse document frequency）\n",
    "- 決定特徵維度與欄位順序\n",
    "\n",
    "transform(...)：用已學好的規則把新資料轉成向量\n",
    "- 只把文本映射到既有詞表的欄位\n",
    "- 用既有的 IDF 權重計算 TF-IDF\n",
    "- 遇到訓練時沒看過的詞（OOV）就忽略（或不產生新欄位）\n",
    "\n",
    "fit_transform(...)：\n",
    "- 等於 fit() + transform() 一次做完\n",
    "'''\n",
    "\n",
    "# 對訓練集進行 TF-IDF 轉換\n",
    "X_train_ft = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# 轉換測試集\n",
    "X_test_t = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立貝葉斯分類器物件\n",
    "# alpha: 平滑參數，預設值為 1.0，愈大表示平滑效果愈強，可避免機率為零的問題，愈小表示較少平滑，較接近原始資料分佈\n",
    "'''\n",
    "平滑就是把「沒看過」改成「幾乎沒看過」，例如：\n",
    "- 沒平滑：沒出現 → 機率 0\n",
    "- 有平滑：沒出現 → 仍給一個很小的機率\n",
    "這樣模型在測試時遇到新詞或稀有詞，不會一票否決某個類別。\n",
    "'''\n",
    "clf = ComplementNB(alpha=1.0)\n",
    "\n",
    "# 訓練模型\n",
    "clf = clf.fit(X_train_ft, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b9b6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 進行預測\n",
    "y_pred = clf.predict(X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ac30ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.722007722007722"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 輸出準確率\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
