{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 掛載 Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYtVvEwzngbn"},"outputs":[],"source":["!pip install google.colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qn9TXerTole6"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{},"source":["# 觀看系統設定"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGSOsvEIpswK"},"outputs":[],"source":["!lsb_release -a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SI-_WjfwUA1"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7V-wNylwglC"},"outputs":[],"source":["!nvcc -V"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R04dIS9QqD0o"},"outputs":[],"source":["# 切換目錄 (Colab 預設目錄為 /content，使用 %cd 切換目錄)\n","%cd /content/drive/My Drive/colab_test/"]},{"cell_type":"markdown","metadata":{},"source":["# 微調模型"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 安裝套件\n","!pip install torch torchvision torchaudio transformers datasets evaluate accelerate scikit-learn scikit-learn "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ROCfoeHI9ak0"},"outputs":[],"source":["import torch\n","print(torch.cuda.is_available())\n","print(torch.cuda.current_device())\n","print(torch.cuda.device(0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEVAYBkXxzkH"},"outputs":[],"source":["'''\n","AutoTokenizer：這有助於將我們的文字資料標記為 BERT 可以理解的格式。 「Auto」前綴意味著它可以為各種模型推斷適當的分詞器。\n","\n","DataCollat​​eWithPadding：確保我們分詞化後的資料，以一致的長度串接在一起，並在必要時增加 padding。這對於訓練的穩定性和效率至關重要。\n","\n","AutoModelForSequenceClassification：一個通用的類別，是用於「序列分類」任務的模型架構。「Auto」前綴使其在各種預訓練模型中具有通用性。\n","\n","TrainingArguments：定義訓練配置的設定，例如 learning rateb、batch size 和 epoch。\n","\n","Trainer：用於訓練和評估，使 finetune 變得簡單。\n","\n","pipeline：使用模型的模型。\n","'''\n","from datasets import load_dataset, load_metric, Dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n",")\n","\n","import random\n","from sklearn.metrics import f1_score\n","\n","\n","'''\n","函式\n","'''\n","# 讀取 .txt 文件\n","def load_dataset_from_file(file_path, seed=42):\n","    # 讀檔\n","    with open(file_path, \"r\", encoding='utf-8') as file:\n","        # 將每一行資料以 list 型態回傳\n","        lines = file.readlines()\n","\n","        # 洗牌 (記得設定 random seed，確保每次洗牌結果一樣)\n","        random.seed(seed)\n","        random.shuffle(lines)\n","\n","        # 整合訓練資料\n","        sentences = []\n","        labels = []\n","        for line in lines:\n","            parts = line.strip().split('\\t')\n","            if len(parts) == 2:\n","                sentences.append(parts[0])\n","                labels.append(int(parts[1]))\n","            else:\n","                print(f'格式錯誤的行號: {line}')\n","        return sentences, labels\n","    \n","# 轉換成 huggingface trainer 可以使用的 datasets\n","def convert_to_dataset(sentences, labels, tokenizer, max_length):\n","    # 建立 Dataset\n","    dataset = Dataset.from_dict({\n","        'sentences': sentences,\n","        'labels': labels\n","    })\n","\n","    # 回傳切分資料 (訓練 和 驗證)\n","    dataset = dataset.train_test_split(test_size=0.2)\n","    '''\n","    DatasetDict({\n","        train: Dataset({\n","            features: ['sentences', 'labels'],\n","            num_rows: 6212\n","        })\n","        test: Dataset({\n","            features: ['sentences', 'labels'],\n","            num_rows: 1554\n","        })\n","    })\n","    '''\n","\n","    # 預處理資料\n","    def preprocess_data(dataset):\n","        # 將句子轉換為 token (tokenization)\n","        return tokenizer(dataset['sentences'], truncation=True, padding=True, return_tensors='pt', max_length=max_length)\n","\n","    # 轉換資料\n","    train_data = dataset['train'].map(preprocess_data, batched=True)\n","    valid_data = dataset['test'].map(preprocess_data, batched=True)\n","\n","    return DatasetDict({\n","        'train': train_data,\n","        'test': valid_data\n","    })\n","\n","# 計算模型評估指標\n","def compute_metrics(predicted_results):\n","    labels = predicted_results.label_ids\n","    preds = predicted_results.predictions.argmax(-1)\n","    f1 = f1_score(labels, preds, average='micro') # binary, micro, macro, weighted\n","    return {\n","        'f1': f1,\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3epMxp60-Y4r"},"outputs":[],"source":["# 主程式 - 微調模型\n","if __name__ == \"__main__\":\n","    '''\n","    設定 hyperparameters\n","    '''\n","    batch_size = 8 # 批次大小\n","    learning_rate = 0.00005 # 學習率 5e-5\n","    epochs = 3 # 訓練次數\n","    model_name = 'bert-base-chinese' # 預訓練模型名稱\n","    output_dir = 'output' # 輸出資料夾\n","    seed = 42 # 隨機種子\n","    max_seq_length = 512 # 最大長度\n","    num_labels = 2 # 二元分類\n","\n","    # 讀取訓練資料\n","    sentences, labels = load_dataset_from_file('./reviews.txt')\n","\n","    # 載入 tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    # 將資料轉換為 huggingface 可以使用的格式\n","    dataset = convert_to_dataset(sentences, labels, tokenizer, max_length=max_seq_length)\n","\n","    # 讀取模型\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n","\n","    # 設定訓練參數\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        overwrite_output_dir=True,\n","        num_train_epochs=epochs,\n","        per_device_train_batch_size=batch_size,\n","        per_device_eval_batch_size=batch_size,\n","        gradient_accumulation_steps=2,\n","        learning_rate=learning_rate,\n","        warmup_steps=50,\n","        weight_decay=0.01,\n","        eval_strategy=\"steps\", # epoch, steps, no\n","        eval_steps=50,\n","        save_strategy=\"steps\", # epoch, steps, no\n","        save_steps=50,\n","        save_total_limit=3,\n","        load_best_model_at_end=True,\n","        seed=seed,\n","        # lr_scheduler_type=\"linear\", # https://blog.csdn.net/muyao987/article/details/139319466\n","        # report_to='wandb', # https://wandb.ai/\n","    )\n","\n","    # 設定 Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=dataset['train'],\n","        eval_dataset=dataset['test'],\n","        data_collator=None, # DataCollatorWithPadding(tokenizer),\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    # 開始訓練\n","    trainer.train()\n","\n","    # 儲存模型\n","    trainer.save_model(output_dir)\n","\n","    # 儲存 tokenizer\n","    tokenizer.save_pretrained(output_dir)"]},{"cell_type":"markdown","metadata":{},"source":["# 拿微調好的模型，進行預測"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    pipeline,\n",")\n","from pprint import pprint\n","\n","model_dir = './output'\n","model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n","tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","pipe = pipeline(task='text-classification', model=model, tokenizer=tokenizer, device=0)\n","\n","list_text = [\n","    '這個房間真的不錯，服務人員也很親切，下次還會再來！',\n","    '這個房間真的很爛，服務人員也很差，下次不會再來！',\n","    '一般般',\n","]\n","result = pipe(list_text)\n","pprint(result)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMKWLyaNDZnqlM6q3MHE3gZ","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
