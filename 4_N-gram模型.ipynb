{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4db1f3e7",
   "metadata": {},
   "source": [
    "# 簡單用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a5ad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\darren\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.604 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '喜歡']\n",
      "['喜歡', '閱讀']\n",
      "['閱讀', '書籍']\n",
      "['書籍', '，']\n",
      "['，', '也']\n",
      "['也', '喜歡']\n",
      "['喜歡', '使用']\n",
      "['使用', '電腦來']\n",
      "['電腦來', '學習']\n",
      "['學習', '新']\n",
      "['新', '的']\n",
      "['的', '知識']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "def generate_ngrams(words, n):\n",
    "    # 建立空的 n-grams 列表\n",
    "    ngrams = []\n",
    "\n",
    "    # 迭代詞彙列表中的每個詞\n",
    "    for i in range(len(words) - n + 1):\n",
    "        # 新增下一個 n-gram\n",
    "        ngrams.append(words[i:i+n])\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "# 範例輸入\n",
    "text = \"我喜歡閱讀書籍，也喜歡使用電腦來學習新的知識\"\n",
    "\n",
    "# 使用 jieba 進行斷詞\n",
    "words = jieba.lcut(text)\n",
    "\n",
    "# 產生 bi-grams\n",
    "bigrams = generate_ngrams(words, 2)\n",
    "\n",
    "# 輸出結果\n",
    "for bigram in bigrams:\n",
    "    print(bigram)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd6613bb",
   "metadata": {},
   "source": [
    "# 計算下一個字出現的機率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b7b5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('蘋果', 0.5), ('橘子', 0.5)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# 假設我們已經有了一個已斷詞的文本列表\n",
    "tokens = ['我', '愛', '吃', '蘋果', '，', '我', '也', '愛', '吃', '橘子']\n",
    "\n",
    "# 建立一個預設為空列表的詞典\n",
    "ngram_dict = defaultdict(Counter)\n",
    "\n",
    "# 指定我們要使用的 N-gram 的 N 值\n",
    "N = 2\n",
    "\n",
    "# 遍歷所有的詞彙\n",
    "for i in range(len(tokens)-N):\n",
    "    # 得到 N-gram 和它的下一個詞彙\n",
    "    ngram = tuple(tokens[i:i + N])\n",
    "    next_token = tokens[i + N]\n",
    "    # 更新詞典\n",
    "    ngram_dict[ngram][next_token] += 1\n",
    "\n",
    "# 轉換次數為機率\n",
    "for ngram, next_tokens in ngram_dict.items():\n",
    "    total_count = sum(next_tokens.values())\n",
    "    for next_token, count in next_tokens.items():\n",
    "        ngram_dict[ngram][next_token] = count / total_count\n",
    "\n",
    "# 假設我們要預測 \"我愛\" 的下一個詞彙\n",
    "ngram = ('愛', '吃')\n",
    "\n",
    "# 從詞典中取得所有可能的下一個詞彙和它們的機率\n",
    "next_tokens_probs = ngram_dict[ngram]\n",
    "\n",
    "# 將它們按照機率排序，取前 k 個\n",
    "k = 2\n",
    "top_k_next_tokens = sorted(next_tokens_probs.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "print(top_k_next_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3@nlp",
   "language": "python",
   "name": "k_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
