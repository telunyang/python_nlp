{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4db1f3e7",
   "metadata": {},
   "source": [
    "# 簡單用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a5ad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\darren\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.320 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('我', '喜歡')\n",
      "('喜歡', '閱讀')\n",
      "('閱讀', '書籍')\n",
      "('書籍', '，')\n",
      "('，', '也')\n",
      "('也', '喜歡')\n",
      "('喜歡', '使用')\n",
      "('使用', '電腦')\n",
      "('電腦', '來')\n",
      "('來', '學習')\n",
      "('學習', '新')\n",
      "('新', '的')\n",
      "('的', '知識')\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "def generate_ngrams(words, n):\n",
    "    # 建立空的 n-grams 列表\n",
    "    ngrams = []\n",
    "\n",
    "    # 迭代詞彙列表中的每個詞\n",
    "    for i in range(len(words) - n + 1):\n",
    "        # 新增下一個 n-gram\n",
    "        ngrams.append(tuple(words[i:i+n]))\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "# 範例輸入\n",
    "text = \"我喜歡閱讀書籍，也喜歡使用電腦來學習新的知識\"\n",
    "\n",
    "# 使用 jieba 進行斷詞\n",
    "words = jieba.lcut(text)\n",
    "\n",
    "# 產生 bi-grams\n",
    "bigrams = generate_ngrams(words, 2)\n",
    "\n",
    "# 輸出結果\n",
    "for bigram in bigrams:\n",
    "    print(bigram)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd6613bb",
   "metadata": {},
   "source": [
    "# 計算下一個字出現的機率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b7b5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('蘋果', 0.5), ('橘子', 0.5)]\n"
     ]
    }
   ],
   "source": [
    "# 假設我們已經有了一個已斷詞的文本列表\n",
    "tokens = ['我', '愛', '吃', '蘋果', '，', '我', '也', '愛', '吃', '橘子']\n",
    "\n",
    "# 建立一個預設為空列表的詞典\n",
    "ngram_dict = {}\n",
    "\n",
    "# 指定我們要使用的 N-gram 的 N 值\n",
    "N = 2\n",
    "\n",
    "# 遍歷所有的詞彙\n",
    "for i in range(len(tokens)-N):\n",
    "    # 得到 N-gram 和它的下一個詞彙\n",
    "    ngram = tuple(tokens[i:i+N])\n",
    "    next_token = tokens[i+N]\n",
    "\n",
    "    # 更新詞典\n",
    "    if ngram not in ngram_dict:\n",
    "        ngram_dict[ngram] = {}\n",
    "    if next_token not in ngram_dict[ngram]:\n",
    "        ngram_dict[ngram][next_token] = 0\n",
    "    ngram_dict[ngram][next_token] += 1\n",
    "\n",
    "# 轉換次數為機率: dict.items() 會回傳一個 (key, value) 的 tuple\n",
    "for ngram, next_tokens in ngram_dict.items():\n",
    "    '''\n",
    "    ngram 是 tuple，\n",
    "    next_tokens 是一個 dict，key 是下一個詞彙，value 是出現的次數。\n",
    "\n",
    "    ngram, next_tokens 的內容如下：\n",
    "    ('我', '愛') {'吃': 1}\n",
    "    ('愛', '吃') {'蘋果': 1, '橘子': 1}\n",
    "    ('吃', '蘋果') {'，': 1}\n",
    "    ('蘋果', '，') {'我': 1}\n",
    "    ('，', '我') {'也': 1}\n",
    "    ('我', '也') {'愛': 1}\n",
    "    ('也', '愛') {'吃': 1}\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    next_tokens.values() 會回傳一個 dict_values 物件，裡面是所有的次數:\n",
    "    dict_values([1])\n",
    "    dict_values([1, 1])\n",
    "    dict_values([1])\n",
    "    dict_values([1])\n",
    "    dict_values([1])\n",
    "    dict_values([1])\n",
    "    dict_values([1])\n",
    "    '''\n",
    "    total_count = sum(next_tokens.values())\n",
    "\n",
    "    # 將次數轉換為機率: dict.items() 會回傳一個 (key, value) 的 tuple\n",
    "    for next_token, count in next_tokens.items():\n",
    "        '''\n",
    "        ngram 是 tuple，\n",
    "        next_tokens 是一個 dict，key 是下一個詞彙，value 是出現的次數。\n",
    "        '''\n",
    "        ngram_dict[ngram][next_token] = count / total_count\n",
    "\n",
    "# 假設我們想要預測愛吃之後的下一個詞彙\n",
    "ngram = ('愛', '吃')\n",
    "\n",
    "# 從詞典中取得所有可能的下一個詞彙和它們的機率\n",
    "next_tokens_probs = ngram_dict[ngram]\n",
    "'''\n",
    "next_tokens_probs 的內容如下：\n",
    "{'蘋果': 0.5, '橘子': 0.5}\n",
    "'''\n",
    "\n",
    "# 將它們按照機率排序，取前 k 個\n",
    "k = 2\n",
    "top_k_next_tokens = sorted(\n",
    "    next_tokens_probs.items(), # dict.items() 會回傳一個 (key, value) 的 tuple\n",
    "    key=lambda x: x[1], \n",
    "    reverse=True\n",
    ")[:k]\n",
    "\n",
    "print(top_k_next_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d67775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3448275862068966\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# 雅卡爾指數\n",
    "https://zh.wikipedia.org/zh-tw/雅卡尔指数\n",
    "'''\n",
    "\n",
    "# 使用 N-gram 計算兩個句子的相似度\n",
    "def jaccard_similarity(s1, s2, N):\n",
    "    # 使用 jieba 進行斷詞\n",
    "    words1 = jieba.lcut(s1)\n",
    "    words2 = jieba.lcut(s2)\n",
    "\n",
    "    # 產生 N-grams\n",
    "    ngrams1 = generate_ngrams(words1, N)\n",
    "    ngrams2 = generate_ngrams(words2, N)\n",
    "\n",
    "    # 計算兩個句子的相似度\n",
    "    common_ngrams = set(ngrams1) & set(ngrams2) # 交集\n",
    "    ngrams_union = set(ngrams1) | set(ngrams2) # 聯集\n",
    "\n",
    "    # 處理分母為 0 的情況\n",
    "    if len(ngrams_union) == 0:\n",
    "        return 0\n",
    "\n",
    "    # 透過計算共同 N-grams 的數量來計算相似度\n",
    "    similarity = len(common_ngrams) / len(ngrams_union)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "# 範例\n",
    "s1 = \"在遙遠的東方，有一個美麗的國度，那裡山清水秀，人民勤勞智慧。\"\n",
    "s2 = \"在很遠的東方，有一個美麗的國家，那裡山明水秀，人民勤勞聰明。\"\n",
    "\n",
    "# 使用 N-gram 來計算兩句話的相似度\n",
    "similarity = jaccard_similarity(s1, s2, 2)\n",
    "print(similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
